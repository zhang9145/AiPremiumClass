from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

#加载鸢尾花数据集
X,y = load_iris(return_X_y=True)
X = X[50:150]
y = y[50:150]

# 数据拆分
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)

# 初始化参数
# 权重参数
theta = np.random.randn(1,4) 
bias = 0
# 超参数
lr = 0.001  #学习率
epochs = 5000  # 训练次数

# 模型计算函数
def forward(x, theta, bias):
    # 线性运算
    z = np.dot(theta, x.T) + bias # shape (90,4)
    # sigmoid
    y_hat = 1 / (1 + np.exp(-z))  # shape (90,4)
    return y_hat

# 计算损失函数
def loss(y, y_hat):
    e = 1e-6
    return - y * np.log(y_hat + e) - (1 - y) * np.log(1 - y_hat + e)

# 计算梯度
def calc_gradient(x,y,y_hat):
    # 计算梯度
    m = x.shape[-1]
    # theta梯度计算
    delta_theta = np.dot((y_hat - y), x) / m
    # bias梯度计算
    delta_bias = np.mean(y_hat - y)
    # 返回梯度
    return delta_theta, delta_bias

# 模型训练
for i in range(epochs):
    # 前向计算
    y_hat = forward(X_train, theta, bias)
    # 计算损失
    loss_val = loss(y_train, y_hat)
    # 计算梯度
    delta_theta, delta_bias = calc_gradient(X_train, y_train, y_hat)
    # 更新参数
    theta = theta - lr * delta_theta
    bias = bias - lr * delta_bias

    if i % 200 == 0:
        # 计算准确率
        acc = np.mean(np.round(y_hat) == y_train)  # [False,True,...,False] -> [0,1,...,0]
        print(f"epoch: {i}, loss: {np.mean(loss_val)}, acc: {acc}")

# 保存模型参数
np.savez('model_params.npz', theta_hat=theta, bias_hat=bias)

# 加载模型参数
#np.loadtxt('model_params.txt',theta=theta,bias=bias)
params = np.load('model_params.npz')
theta_hat = params['theta_hat']
bias_hat = params['bias_hat']

# 用测试集试验模型
idx = np.random.randint(len(X_test)) # 随机选择一个测试样本索引
x = X_test[idx]
y = y_test[idx]
# 代入模型，查看实际值和预测值
predict = np.round(forward(x, theta_hat, bias_hat))
print(f"y: {y}, predict: {predict}")
