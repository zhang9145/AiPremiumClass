# 导入需要的模块
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.transforms.v2 import ToTensor  # 转换图像数据为张量
from torchvision.datasets import KMNIST  
from torch.utils.data import DataLoader  # 数据加载器

# 加载数据集
train_data = KMNIST(root='./KMNIST_data', train=True, download=True, transform=ToTensor())
test_data = KMNIST(root='./KMNIST_data', train=False, download=True, transform=ToTensor())

# 定义不同的超参数
hidden_sizes_list = [[128], [256, 128], [512, 256, 128]]
learning_rates = [0.001, 0.01, 0.1]
batch_sizes = [64, 128, 256]
epochs_list = [5, 10, 20]  # 定义不同的 epochs 值

# 循环尝试不同的超参数
for hidden_sizes in hidden_sizes_list: # 遍历不同的隐藏层规模
    for lr in learning_rates:  # 遍历不同的学习率
        for batch_size in batch_sizes:  # 遍历不同的 batch_sizes
            for epochs in epochs_list:  # 遍历不同的 epochs
                # 创建数据加载器
                train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
                test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

                # 定义模型
                layers = []
                input_size = 784
                for hidden_size in hidden_sizes:
                    layers.append(nn.Linear(input_size, hidden_size))
                    layers.append(nn.Sigmoid())
                    input_size = hidden_size
                layers.append(nn.Linear(input_size, 10))
                model = nn.Sequential(*layers)

                # 损失函数&优化器
                loss_fn = nn.CrossEntropyLoss()  # 交叉熵损失函数
                optimizer = optim.SGD(model.parameters(), lr=lr)  # 优化器（模型参数更新）

                # 训练模型
                for epoch in range(epochs):  # 使用当前的 epochs 值
                    for data, target in train_loader:
                        # 前向运算
                        output = model(data.reshape(-1, 784))
                        # 计算损失
                        loss = loss_fn(output, target)
                        # 反向传播
                        optimizer.zero_grad()  # 所有参数梯度清零
                        loss.backward()  # 计算梯度（参数.grad）
                        optimizer.step()  # 更新参数

                    print(f"Hidden Sizes: {hidden_sizes}, LR: {lr}, Batch Size: {batch_size}, Epochs: {epochs}, Epoch: {epoch+1}, Train Loss: {loss.item()}")

                # 测试
                correct = 0
                total = 0
                with torch.no_grad():  # 不计算梯度
                    for data, target in test_loader:
                        output = model(data.reshape(-1, 784))
                        _, predicted = torch.max(output, 1)  # 返回每行最大值和索引
                        total += target.size(0)  # size(0) 等效 shape[0]
                        correct += (predicted == target).sum().item()

                print(f'Accuracy: {correct/total*100}%')
